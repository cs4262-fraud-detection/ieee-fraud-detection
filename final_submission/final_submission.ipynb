{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import preprocessing, decomposition, model_selection, linear_model, metrics, ensemble, svm, utils\n",
    "from sklearn.datasets import make_classification\n",
    "import gc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import metrics\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "transactions = pd.read_csv('data/train_transaction.csv')\n",
    "identities = pd.read_csv('data/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "dataset = transactions.merge(identities, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce memory usage\n",
    "def reduce_mem(df):\n",
    "  start_mem=df.memory_usage().sum()/1024**2\n",
    "  print('Initial Memory Usage : {:.2f} MB'.format(start_mem))\n",
    "  for col in df.columns:\n",
    "    col_type=df[col].dtype\n",
    "    if col_type != object:\n",
    "      mn, mx = df[col].min(), df[col].max()\n",
    "      if str(col_type)[:3]=='int':\n",
    "        if mn>np.iinfo(np.int8).min and mx<np.iinfo(np.int8).max:\n",
    "          df[col]=df[col].astype(np.int8)\n",
    "        elif mn>np.iinfo(np.int16).min and mx<np.iinfo(np.int16).max:\n",
    "          df[col]=df[col].astype(np.int16)\n",
    "        elif mn>np.iinfo(np.int32).min and mx<np.iinfo(np.int32).max:\n",
    "          df[col]=df[col].astype(np.int32)\n",
    "      else:\n",
    "        if mn>np.finfo(np.float16).min and mx<np.finfo(np.float16).max:\n",
    "          df[col]=df[col].astype(np.float16)\n",
    "        elif mn>np.finfo(np.float32).min and mx<np.finfo(np.float32).max:\n",
    "          df[col]=df[col].astype(np.float32)\n",
    "  end_mem = df.memory_usage().sum()/1024**2\n",
    "  print('Final Memory Usage : {:.2f} MB'.format(end_mem))\n",
    "  print('Decreased by {:.2f}%'.format(100*(start_mem-end_mem)/start_mem))\n",
    "  return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Add source for the reduce_mem code snippet? Should we include this on final submission?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage : 1959.88 MB\n",
      "Final Memory Usage : 648.22 MB\n",
      "Decreased by 66.93%\n"
     ]
    }
   ],
   "source": [
    "dataset = reduce_mem(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del transactions, identities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display some sort of visualization of the data (e.g. show that is imbalanced)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "not_fraud_downsampled = utils.resample(not_fraud, replace=False, n_samples = len(fraud), random_state = 27) \n",
    "sownsampled_dataset = pd.concat([not_fraud_downsampled, fraud])\n",
    "'''\n",
    "# Note: Downsampling does not seem to improve model results, so I don't think it's worth doing it. However, we\n",
    "# should definitely mention on the final presentation/paper that we tried it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Drop columns with too many NaN values?\n",
    "'''\n",
    "def drop_columns(dataset, threshold):\n",
    "    columns_to_drop = []\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].isna().sum()/dataset.shape[0] < threshold:\n",
    "            columns_to_drop.append(column)\n",
    "    return dataset.drop(columns_to_drop, axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values\n",
    "dataset = dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(dataset[col].values))\n",
    "        dataset[col] = lbl.transform(list(dataset[col].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.isFraud.values\n",
    "dataset = dataset.drop('isFraud',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "dataset_columns = list(dataset.columns)\n",
    "dataset[dataset_columns] = preprocessing.StandardScaler().fit_transform(dataset[dataset_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_pca = dataset[dataset_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6418335062508921\n"
     ]
    }
   ],
   "source": [
    "# Reduce number of dimensions through PCA\n",
    "reduced_number_of_dimensions = 15 # Note: This number should be increased before making the final\n",
    "# submission. Lower values lead to faster model training but lower model accuracy/f1-scores.\n",
    "pca = decomposition.PCA(n_components=reduced_number_of_dimensions, random_state=42)\n",
    "X_post_pca = pca.fit_transform(X_pre_pca) \n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 80/20 train/test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_post_pca, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function returns a model with the hyperparameters that yield the best f1 score\n",
    "def get_model_with_best_estimators(model, parameters, X_train, y_train):\n",
    "    grid_search = model_selection.GridSearchCV(model, parameters, scoring='f1', cv=4)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_parameters = {'n_estimators':[50, 100, 200], 'criterion':('gini', 'entropy')}\n",
    "random_forest = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "best_random_forest_model = get_model_with_best_estimators(random_forest, random_forest_parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_parameters = {'C':[1, 10], 'penalty':('l2',), 'solver':('saga', 'newton-cg')}\n",
    "logistic_regression = linear_model.LogisticRegression(n_jobs=-1, max_iter=200)\n",
    "best_logistic_regression_model = get_model_with_best_estimators(logistic_regression, logistic_regression_parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svm = svm.SVC(max_iter=200)\n",
    "best_svm_model = get_model_with_best_estimators(svm, svm_parameters, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113974\n",
      "           1       0.88      0.30      0.45      4134\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.93      0.65      0.72    118108\n",
      "weighted avg       0.97      0.97      0.97    118108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, best_random_forest_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    113974\n",
      "           1       0.67      0.07      0.13      4134\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.82      0.54      0.56    118108\n",
      "weighted avg       0.96      0.97      0.95    118108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, best_logistic_regression_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.76      0.85    113974\n",
      "           1       0.06      0.44      0.11      4134\n",
      "\n",
      "    accuracy                           0.75    118108\n",
      "   macro avg       0.52      0.60      0.48    118108\n",
      "weighted avg       0.94      0.75      0.83    118108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, best_svm_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_PCA(k, x_tr, y):\n",
    "    X_pca = decomposition.PCA(n_components=k).fit_transform(x_tr)  \n",
    "    return model_selection.train_test_split(X_pca, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=50\n",
      "Train on 467432 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "467432/467432 [==============================] - 7s 15us/sample - loss: 0.1512 - accuracy: 0.9639 - categorical_accuracy: 1.0000 - val_loss: 0.1075 - val_accuracy: 0.9726 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1196 - accuracy: 0.9702 - categorical_accuracy: 1.0000 - val_loss: 0.1057 - val_accuracy: 0.9724 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1154 - accuracy: 0.9709 - categorical_accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 0.9726 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1124 - accuracy: 0.9713 - categorical_accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9724 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1104 - accuracy: 0.9717 - categorical_accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9720 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1088 - accuracy: 0.9720 - categorical_accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9736 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1070 - accuracy: 0.9722 - categorical_accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9744 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "467432/467432 [==============================] - 6s 12us/sample - loss: 0.1060 - accuracy: 0.9723 - categorical_accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9746 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1048 - accuracy: 0.9727 - categorical_accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9756 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1036 - accuracy: 0.9730 - categorical_accuracy: 1.0000 - val_loss: 0.0977 - val_accuracy: 0.9750 - val_categorical_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113974\n",
      "           1       0.83      0.32      0.46      4134\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.90      0.66      0.72    118108\n",
      "weighted avg       0.97      0.97      0.97    118108\n",
      "\n",
      "For k=100\n",
      "Train on 467432 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "467432/467432 [==============================] - 7s 15us/sample - loss: 0.1612 - accuracy: 0.9575 - categorical_accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9714 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1162 - accuracy: 0.9705 - categorical_accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9732 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1103 - accuracy: 0.9717 - categorical_accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9748 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1067 - accuracy: 0.9723 - categorical_accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9752 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.1038 - accuracy: 0.9730 - categorical_accuracy: 1.0000 - val_loss: 0.0942 - val_accuracy: 0.9752 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.1013 - accuracy: 0.9735 - categorical_accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9766 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.0993 - accuracy: 0.9741 - categorical_accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9760 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0974 - accuracy: 0.9745 - categorical_accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9778 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.0961 - accuracy: 0.9750 - categorical_accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9758 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.0939 - accuracy: 0.9753 - categorical_accuracy: 1.0000 - val_loss: 0.0899 - val_accuracy: 0.9770 - val_categorical_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113974\n",
      "           1       0.86      0.37      0.52      4134\n",
      "\n",
      "    accuracy                           0.98    118108\n",
      "   macro avg       0.92      0.68      0.75    118108\n",
      "weighted avg       0.97      0.98      0.97    118108\n",
      "\n",
      "For k=150\n",
      "Train on 467432 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "467432/467432 [==============================] - 7s 15us/sample - loss: 0.1511 - accuracy: 0.9629 - categorical_accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9724 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "467432/467432 [==============================] - 6s 14us/sample - loss: 0.1124 - accuracy: 0.9710 - categorical_accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9742 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1063 - accuracy: 0.9723 - categorical_accuracy: 1.0000 - val_loss: 0.0964 - val_accuracy: 0.9746 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1026 - accuracy: 0.9729 - categorical_accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9758 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0989 - accuracy: 0.9737 - categorical_accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9768 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0963 - accuracy: 0.9744 - categorical_accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 0.9776 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0938 - accuracy: 0.9749 - categorical_accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9774 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0917 - accuracy: 0.9754 - categorical_accuracy: 1.0000 - val_loss: 0.0881 - val_accuracy: 0.9786 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0893 - accuracy: 0.9759 - categorical_accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9780 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0875 - accuracy: 0.9765 - categorical_accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9770 - val_categorical_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113974\n",
      "           1       0.82      0.43      0.56      4134\n",
      "\n",
      "    accuracy                           0.98    118108\n",
      "   macro avg       0.90      0.71      0.77    118108\n",
      "weighted avg       0.97      0.98      0.97    118108\n",
      "\n",
      "For k=200\n",
      "Train on 467432 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "467432/467432 [==============================] - 7s 15us/sample - loss: 0.1559 - accuracy: 0.9595 - categorical_accuracy: 1.0000 - val_loss: 0.1028 - val_accuracy: 0.9736 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1112 - accuracy: 0.9712 - categorical_accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9742 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1052 - accuracy: 0.9724 - categorical_accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9756 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.1006 - accuracy: 0.9735 - categorical_accuracy: 1.0000 - val_loss: 0.0912 - val_accuracy: 0.9768 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0968 - accuracy: 0.9744 - categorical_accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9774 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0934 - accuracy: 0.9752 - categorical_accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9770 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0912 - accuracy: 0.9757 - categorical_accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9776 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0882 - accuracy: 0.9765 - categorical_accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9776 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0860 - accuracy: 0.9770 - categorical_accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9790 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "467432/467432 [==============================] - 6s 13us/sample - loss: 0.0839 - accuracy: 0.9774 - categorical_accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9782 - val_categorical_accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113974\n",
      "           1       0.86      0.41      0.56      4134\n",
      "\n",
      "    accuracy                           0.98    118108\n",
      "   macro avg       0.92      0.71      0.77    118108\n",
      "weighted avg       0.98      0.98      0.97    118108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural network approach\n",
    "\n",
    "K = [50, 100, 150, 200]\n",
    "nns = [Sequential() for _ in range(len(K))]\n",
    "results = []\n",
    "batch_size = 5000\n",
    "num_epochs = 10\n",
    "\n",
    "for k, cur_nn in zip(K, nns):\n",
    "    n_cols = k\n",
    "    x_tr, x_test, y_tr, y_test = split_with_PCA(k, X_pre_pca, y)\n",
    "    cur_nn.add(Dense(300, activation='relu', input_shape=(n_cols,)))\n",
    "    cur_nn.add(Dropout(0.2))\n",
    "    cur_nn.add(Dense(500, activation='relu'))\n",
    "    cur_nn.add(Dropout(0.2))\n",
    "    cur_nn.add(Dense(100, activation='relu'))\n",
    "    cur_nn.add(Dropout(0.2))\n",
    "    cur_nn.add(Dense(25, activation='relu'))\n",
    "    cur_nn.add(Dropout(0.2))\n",
    "    cur_nn.add(Dense(1, activation='sigmoid'))\n",
    "    cur_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\n",
    "    x_vl, y_vl = x_tr[:batch_size], y_tr[:batch_size]\n",
    "    x_train, y_train = x_tr[batch_size:], y_tr[batch_size:]\n",
    "    print(\"For k=\" + str(k))\n",
    "    cur_nn.fit(x_train, y_train, validation_data=(x_vl, y_vl), epochs=num_epochs, batch_size=batch_size)\n",
    "    # res = cur_nn.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "    print(sklearn.metrics.classification_report(y_test, cur_nn.predict_classes(x_test)))\n",
    "    # results.append(res)\n",
    "    # print('test loss, test acc, categorical accuracy:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
